{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('Q_Table.pkl', 'rb')\n",
    "\n",
    "Q_table = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 5\n",
    "cols = 5\n",
    "a = (0,1)\n",
    "A = (4, 1)\n",
    "b = (0, 3)\n",
    "B = (2, 3)\n",
    "start_state = (np.random.randint(0, rows), np.random.randint(0, cols))\n",
    "n_trails = 100\n",
    "\n",
    "class gridworld:\n",
    "    def __init__(self, position=start_state, n_trails=n_trails):\n",
    "        #duration\n",
    "        self.n_trails = n_trails\n",
    "\n",
    "        #creating the grid\n",
    "        self.grid = np.zeros([rows, cols])\n",
    "        \n",
    "        #states \n",
    "        self.previous_states = [(0,0)]\n",
    "        self.current_state = position\n",
    "        \n",
    "        self.reward = 0\n",
    "        \n",
    "        self.total_rewards = 0\n",
    "        self.all_actions = []\n",
    "        self.all_rewards = []\n",
    "        self.all_states = []\n",
    "        \n",
    "    def move(self, action):\n",
    "        \"\"\"\n",
    "        0 = up\n",
    "        1 = down\n",
    "        2 = left \n",
    "        3 = right\"\"\"\n",
    "        \n",
    "        if action == 0:\n",
    "            next_state = (self.current_state[0] - 1, self.current_state[1])\n",
    "            \n",
    "        elif action == 1:\n",
    "            next_state = (self.current_state[0] + 1, self.current_state[1])\n",
    "            \n",
    "        elif action == 2:\n",
    "            next_state = (self.current_state[0], self.current_state[1] - 1)\n",
    "            \n",
    "        elif action == 3:\n",
    "            next_state = (self.current_state[0], self.current_state[1] + 1)\n",
    "        \n",
    "        if (next_state[0] >= 0) & (next_state[0] <= rows - 1):\n",
    "            if (next_state[1] >= 0) & (next_state[1] <= cols - 1):\n",
    "                return next_state\n",
    "        \n",
    "        return self.current_state\n",
    "\n",
    " \n",
    "    def give_reward(self):\n",
    "        if (self.current_state == a):\n",
    "            self.current_state = A\n",
    "            return 10\n",
    "        \n",
    "        elif (self.current_state == b):\n",
    "            self.current_state = B\n",
    "            return 5\n",
    "        \n",
    "        elif (self.current_state == self.previous_states[-1]):\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def show(self):\n",
    "        temp = np.zeros([rows, cols])\n",
    "        temp[self.current_state] = 1\n",
    "        print(temp)\n",
    "        self.all_states.append(temp)\n",
    "        \n",
    "    def run(self, agent):\n",
    "\n",
    "        for i in range(self.n_trails):\n",
    "            \n",
    "            action = agent.choose()\n",
    "            self.current_state = self.move(action)\n",
    "            \n",
    "            self.reward = self.give_reward()\n",
    "            self.total_rewards += self.reward\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            self.show()\n",
    "            print('old state:{}, action:{}, new state:{}, reward:{}'.format(self.previous_states[-1], action, self.current_state, self.reward))\n",
    "            sleep(.5) \n",
    "            \n",
    "            self.previous_states.append(self.current_state)\n",
    "            self.all_actions.append(action)\n",
    "            self.all_rewards.append(self.reward)\n",
    "            \n",
    "            agent.update()\n",
    "            \n",
    "        return self.total_rewards\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSampliler:\n",
    "    \n",
    "    def __init__(self, env, table_size=None, learning_rate=None, epsilon=None, discount=None):\n",
    "        self.env = env\n",
    "        self.choice = 0\n",
    "        self.state = env.current_state\n",
    "        \n",
    "        self.reward = env.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random(BaseSampliler):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def choose(self):\n",
    "        \n",
    "        self.choice = np.random.randint(0,4)\n",
    "        \n",
    "        return self.choice\n",
    "    def update(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class q_learn(BaseSampliler):\n",
    "    def __init__(self, env, table_size, learning_rate, epsilon, discount):\n",
    "        super().__init__(env, table_size, learning_rate, epsilon, discount)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.start_epsililon= 1 \n",
    "        self.end_epsilon = self.env.n_trails // 2\n",
    "        self.decay = epsilon/(self.end_epsilon - self.start_epsililon)\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.discount = discount\n",
    "        \n",
    "        self.low = np.array((0,0))\n",
    "        self.high = np.array((4,4))\n",
    "        self.table_size = table_size\n",
    "        self.os_size = [self.table_size] * len(self.high)\n",
    "        \n",
    "        #self.qtable = np.random.uniform(low=-1, high=0, size=(self.os_size + [4]))\n",
    "        self.qtable = Q_table\n",
    "        \n",
    "        self.state\n",
    "        \n",
    "    def choose(self):\n",
    "        \n",
    "        if np.random.random() > self.epsilon:\n",
    "            self.choice = np.argmax(self.qtable[self.state])\n",
    "        else:\n",
    "            self.choice = np.random.randint(0, 4)\n",
    "    \n",
    "        return self.choice\n",
    "    \n",
    "    def update(self):\n",
    "        self.reward = self.env.reward\n",
    "        \n",
    "        self.current_q = self.qtable[self.state + (self.choice, )]\n",
    "        \n",
    "        self.state = self.env.current_state\n",
    "        \n",
    "        self.future_q = np.max(self.qtable[self.state])\n",
    "        \n",
    "        self.new_q = (1-self.lr) * self.current_q + self.lr * (self.reward+ self.discount * self.future_q)\n",
    "        \n",
    "        self.qtable[self.state + (self.choice, )] = self.new_q\n",
    "        \n",
    "        self.epsilon -= self.decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "old state:(4, 1), action:3, new state:(4, 2), reward:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en0 = gridworld()\n",
    "r = random(en0)\n",
    "en0.run(agent=r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "old state:(2, 1), action:0, new state:(1, 1), reward:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en1 = gridworld()\n",
    "q = q_learn(en0, table_size=25, learning_rate=.01, epsilon=.1, discount=.1)\n",
    "en0.run(agent=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q_table = q.qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''output = open('Q_Table.pkl', 'wb')\n",
    "pickle.dump(Q_table, output)\n",
    "output.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en0.total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
